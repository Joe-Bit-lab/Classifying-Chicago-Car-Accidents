{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Three CSV Files in this Dataset:\n",
    "The three datasets are broken down into 'Crashes' (the main data file), and 'Vehicles'/'People' (secondary data files). The first task will be to inspect the two secondary files and identify any relevent columns to be imported into the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes = pd.read_csv('Data/Traffic_Crashes_-_Crashes.csv')\n",
    "df_people = pd.read_csv('Data/Traffic_Crashes_-_People.csv',low_memory=False)\n",
    "df_vehicles = df = pd.read_csv('Data/Traffic_Crashes_-_Vehicles.csv',low_memory=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the 'Drivers' file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of overlap between this file and the main file. However the PERSON_TYPE, AGE, SEX, and PHYSICAL_CONDITION seem to contain new and relevant information. We will remove missing values, bin their rows into a binary categories and then encode them ordinally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Missing and Spurious Values:\n",
    "df_people.SEX = df_people.SEX.replace(to_replace=['X','U'],value=np.nan)\n",
    "df_people.PHYSICAL_CONDITION = df_people.PHYSICAL_CONDITION.replace(to_replace='REMOVED BY EMS',value=np.nan)\n",
    "index_to_drop = df_people.loc[df_people.AGE == -49.0].index\n",
    "index_to_drop1 = df_people.loc[df_people.AGE == -1.0].index\n",
    "df_people = df_people.drop(index = index_to_drop)\n",
    "df_people = df_people.drop(index = index_to_drop1)\n",
    "df_people = df_people[['CRASH_RECORD_ID','PERSON_TYPE','AGE','SEX','PHYSICAL_CONDITION']]\n",
    "df_people.SEX = df_people.SEX.fillna(method='pad')\n",
    "df_people.PHYSICAL_CONDITION = df_people.PHYSICAL_CONDITION.fillna(method='pad')\n",
    "df_people.AGE = df_people.AGE.fillna(method='pad')\n",
    "df_people.AGE = df_people.AGE.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the PHYSICAL_CONDIdtype columns into \"Normal\" and \"Incapacitated\" Bins:\n",
    "def label_states (row):\n",
    "    if row['PHYSICAL_CONDITION'] in ['NORMAL','OTHER']:\n",
    "        return 'Normal'\n",
    "    if row['PHYSICAL_CONDITION'] in ['IMPAIRED - ALCOHOL','HAD BEEN DRINKING','IMPAIRED - DRUGS','IMPAIRED - ALCOHOL AND DRUGS','FATIGUED/ASLEEP','EMOTIONAL',\\\n",
    "                                     'ILLNESS/FAINTED','MEDICATED']:\n",
    "        return 'Incapacitated'\n",
    "df_people['PHYSICAL_CONDTION'] = df_people.apply(lambda row: label_states(row), axis=1)\n",
    "\n",
    "#Ordinally Encoding PHYSICAL_CONDITION and \"SEX\" columns\n",
    "ord_enc = OrdinalEncoder()\n",
    "condition_array = df_people.PHYSICAL_CONDITION.values.reshape(-1,1)\n",
    "sex_array = df_people.SEX.values.reshape(-1,1)\n",
    "df_people.SEX = ord_enc.fit_transform(sex_array)\n",
    "df_people.PHYSICAL_CONDITION = ord_enc.fit_transform(condition_array)\n",
    "\n",
    "#Grouping by \"CRASH_RECORD_ID\" and taking the mean so we can merge these columns with the main data file:\n",
    "sex_df = df_people.SEX.groupby(df_people['CRASH_RECORD_ID']).mean()\n",
    "age_df = df_people.AGE.groupby(df_people['CRASH_RECORD_ID']).mean()\n",
    "cond_df = df_people.PHYSICAL_CONDITION.groupby(df_people['CRASH_RECORD_ID']).mean()\n",
    "\n",
    "#Checking the shape to make sure they are all the same size:\n",
    "sex_df.shape, age_df.shape, cond_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating sex_df, age_df, and cond_df into one dataframe that can merge with the main data file:\n",
    "df_people_merge = pd.concat([sex_df,age_df,cond_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the 'Vehicle' file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there seems to be a lot of overlap between this file and the main file. However the NUM_PASSENGERS and VEHICLE_DEFECT seem to contain new and relevant information. Just like the passenger data, we will remove missing values, bin their rows into a binary categories and then encode them ordinally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles.FIRST_CONTACT_POINT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values:\n",
    "df_vehicles = df_vehicles[['CRASH_RECORD_ID','NUM_PASSENGERS','VEHICLE_DEFECT']]\n",
    "df_vehicles['NUM_PASSENGERS'] = df_vehicles.NUM_PASSENGERS.fillna(method='pad')\n",
    "df_vehicles['NUM_PASSENGERS'] = df_vehicles.NUM_PASSENGERS.fillna(method='bfill')\n",
    "df_vehicles['VEHICLE_DEFECT'] = df_vehicles.VEHICLE_DEFECT.fillna(value='UNKOWN')\n",
    "\n",
    "#Binning VEHICLE_DEFECT into \"Normal\" (1) and \"Defective\" (0) bins:\n",
    "def label_states(row):\n",
    "    if row['VEHICLE_DEFECT'] in ['NONE', 'UNKNOWN']:\n",
    "        return 1\n",
    "    if row['VEHICLE_DEFECT'] in ['OTHER','BRAKES','TIRES','STEERING','WHEELS','SUSPENSION','ENGINE/MOTOR','FUEL SYSTEM','LIGHTS','WINDOWS','CARGO','SIGNALS',\\\n",
    "                               'RESTRAINT SYSTEM','TRAILER COUPLING']:\n",
    "        return 0\n",
    "df_vehicles['VEHICLE_DEFECT'] = df_vehicles.apply(lambda row: label_states(row), axis=1)\n",
    "\n",
    "#Grouping by \"CRASH_RECORD_ID\" and taking the mean so we can merge these columns with the main data file:\n",
    "df_num_passengers = df_vehicles.NUM_PASSENGERS.groupby(df_vehicles.CRASH_RECORD_ID).mean()\n",
    "df_defect = df_vehicles.VEHICLE_DEFECT.groupby(df_vehicles.CRASH_RECORD_ID).mean()\n",
    "\n",
    "#Checking the shape to make sure they are all the same size:\n",
    "df_num_passengers.shape, df_defect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating sex_df, age_df, and cond_df into one dataframe that can merge with the main data file:\n",
    "df_vehicles_merge = pd.concat([df_num_passengers,df_defect],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Three Data Files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the secondary data files and selected the rows and columns we want, we need to merge the three dataframes on  \"CRASH_RECORD_ID\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the two secondary files first:\n",
    "vehicles_people = pd.merge(df_vehicles_merge, df_people_merge, how='outer', on='CRASH_RECORD_ID')\n",
    "#Merging the secondary files to the primary file:\n",
    "df = pd.merge(df_crashes, vehicles_people, how='outer', on='CRASH_RECORD_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(df.isnull().sum())\n",
    "percent_null = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in nulls.items():\n",
    "    percent_null.append(j/(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_null = pd.DataFrame(percent_null)\n",
    "percent_null = percent_null.T\n",
    "percent_null.loc[percent_null[0] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "This dataset contains columns that have large numbers of missing values, or do not bear any relevance to the classification target. Below is a table outlining the columns dropped and why:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column|Reason Dropped|Column|Reason Dropped\n",
    "|------|--------------|------|--------------\n",
    "|PHOTOS_TAKEN_I|Too many missing values|CRASH_DATE_EST_I|Too many missing values/Not relevant\n",
    "|STATEMENTS_TAKEN_I|Too many missing values/Not relevant|INJURIES_UNKOWN| Essentially a null column|\n",
    "|DOORING_I|Too many missing values|REPORT_TYPE|Not relevant|                        \n",
    "|WORK_ZONE_I|Too many missing values|RD_NO|Only unique values|                    \n",
    "|WORK_ZONE_TYPE|Too many missing values|DATE_POLICE_NOTIFIED|Not relevant|                   \n",
    "|WORKERS_PRESENT_I|Too many missing values|NOT_RIGHT_OF_WAY_I|Too many missing values|\n",
    "|LANE_CNT|Too many missing values|STREET_DIRECTION|Not relevant|\n",
    "|INTERSECTION_RELATED|Too many missing values|HIT_AND_RUN|Too many missing values\n",
    "|SEC_CONTRIBUTORY_CAUSE|Too many missing values|num_units|Not relevant|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Columns\n",
    "df = df.drop(['PHOTOS_TAKEN_I','STATEMENTS_TAKEN_I','DOORING_I','WORK_ZONE_I','WORK_ZONE_TYPE','WORKERS_PRESENT_I','LANE_CNT','CRASH_DATE_EST_I',\\\n",
    "              'REPORT_TYPE','RD_NO','DATE_POLICE_NOTIFIED','NOT_RIGHT_OF_WAY_I','STREET_DIRECTION','INJURIES_UNKNOWN','INTERSECTION_RELATED_I',\\\n",
    "              'HIT_AND_RUN_I','NUM_UNITS'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the type and count of values within all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking the value_counts of all remaining columns:\n",
    "# for i,j in df.items():\n",
    "#     print('------')\n",
    "#     print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Null values: \n",
    "There are multiple columns with string values such as 'UNKNOWN' ,'UNABLE TO DETERMINE' etc. These are essentially null values for those columns as they contain no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing location of null values:\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dropping Rows with null values:\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Lowercase:\n",
    "So that we're not constantly pressings caps lock or shift, it is useful to convert all column names and strings to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making things lowercase\n",
    "df.columns = df.columns.map(lambda x: x.lower())\n",
    "df_str = df.select_dtypes(include=object)\n",
    "df_num = df.select_dtypes(exclude=object)\n",
    "df_str = df_str.astype(str)\n",
    "df_str = df_str.applymap(lambda x: x.lower())\n",
    "df = pd.concat([df_str, df_num], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 'crash_date' Column to a DateTime object:\n",
    "As a consequnce of the previous cell of code, the 'crash_date' column was converted into a string, it is probably more useful to have this column stored as a DateTime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting crash_date as DateTime Object\n",
    "df.crash_date = pd.to_datetime(df.crash_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = df.duplicated()\n",
    "dup.value_counts()\n",
    "#There appear to be no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Cleaned Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Traffic_Crashes_Cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
